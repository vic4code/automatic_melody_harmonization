{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.autograd import Variable\n",
    "from model.m2m_CVAE import CVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loss function\n",
    "def loss_fn(loss_function, logp, target, length, mean, log_var, anneal_function, step, k, x0):\n",
    "\n",
    "    # Negative Log Likelihood\n",
    "    NLL_loss = loss_function(logp, target)\n",
    "\n",
    "    # KL Divergence\n",
    "    KL_loss = -0.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp())\n",
    "    KL_weight = kl_anneal_function(anneal_function, step, k, x0)\n",
    "\n",
    "    return NLL_loss, KL_loss, KL_weight\n",
    "\n",
    "## Annealing function \n",
    "def kl_anneal_function(anneal_function, step, k, x0):\n",
    "        if anneal_function == 'logistic':\n",
    "            return float(1/(1+np.exp(-k*(step-x0))))\n",
    "        elif anneal_function == 'linear':\n",
    "            return min(1, step/x0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  1080.3507080078125\n"
     ]
    }
   ],
   "source": [
    "step = 1\n",
    "batch_size = 5\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "# Annealing parameter\n",
    "k = 0.0025\n",
    "x0 = 2500\n",
    "\n",
    "loss_function = torch.nn.NLLLoss(reduction='sum')\n",
    "\n",
    "model = CVAE().to(device)\n",
    "\n",
    "melody = torch.randn(batch_size, 272, 12 * 24 * 2)\n",
    "chord_onehot = torch.randn(batch_size, 272, 96)\n",
    "length = torch.Tensor([272, 100, 200, 150, 120]).long()\n",
    "\n",
    "melody_pred, logp ,mu, log_var, _ = model(chord_onehot,melody,length)\n",
    "\n",
    "# Arrange \n",
    "pred_flatten = []\n",
    "groundtruth_flatten = []\n",
    "logp_flatten = []\n",
    "length = length.squeeze()\n",
    "\n",
    "for i in range(batch_size):\n",
    "\n",
    "    # Get predicted softmax chords by length of the song (cutting off padding 0), (1,length,96)\n",
    "    logp_flatten.append(logp[i][:length[i]])\n",
    "\n",
    "    # Get predicted softmax chords by length of the song (cutting off padding 0), (1,length,12 * 24 * 2)\n",
    "    pred_flatten.append(melody_pred[i][:length[i]])\n",
    "\n",
    "    # Get groundtruth chords by length of the song (cutting off padding 0), (1,length)\n",
    "    groundtruth_flatten.append(melody[i][:length[i]])\n",
    "\n",
    "# Rearrange for loss calculatio\n",
    "logp_flatten = torch.cat(logp_flatten, dim=0)\n",
    "pred_flatten = torch.cat(pred_flatten, dim=0)\n",
    "groundtruth_flatten = torch.cat(groundtruth_flatten,dim=0).long()\n",
    "groundtruth_index = torch.max(groundtruth_flatten,1).indices\n",
    "\n",
    "# loss calculation\n",
    "# Add weight to NLL also\n",
    "NLL_loss, KL_loss, KL_weight = loss_fn(loss_function = loss_function, logp = logp_flatten, target = groundtruth_index, length = length, mean = mu, log_var = log_var,anneal_function='logistic', step=step, k=k, x0=x0)\n",
    "# NLL_loss, KL_loss, KL_weight = loss_fn(logp = chord_pred_flatten, target = chord_groundtruth_index, length = length, mean = mu, log_var = log_var,anneal_function='logistic', step=step, k=k, x0=x0)\n",
    "\n",
    "loss = (NLL_loss + KL_weight * KL_loss) / batch_size\n",
    "print('loss: ',loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
