{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/md1/user_victor/automatic_melody_harmonization'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tonal import pianoroll2number, joint_prob2pianoroll96\n",
    "from tonal import tonal_centroid, chord482note, chord962note, note2number\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import pickle\n",
    "from model.surprisingness_CVAE import CVAE\n",
    "from decode import *\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melody_framewise = np.load('./data/melody_data.npy')\n",
    "chord_groundtruth_idx = np.load('./data/chord_groundtruth.npy')\n",
    "\n",
    "melody = np.load('./data/melody_baseline.npy')\n",
    "chord = np.load('./data/number_96.npy')\n",
    "chord_onehot = np.load('./data/onehot_96.npy')\n",
    "length = np.load('./data/length.npy')\n",
    "\n",
    "f = open('./data/tempos', 'rb')\n",
    "tempos = pickle.load(f)\n",
    "f.close()\n",
    "f = open('./data/downbeats', 'rb')\n",
    "downbeats = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitting testing set...\n"
     ]
    }
   ],
   "source": [
    "val_size = 500\n",
    "print('splitting testing set...')\n",
    "val_melody_framewise = melody_framewise[:val_size]\n",
    "val_chord_groundtruth_idx = chord_groundtruth_idx[:val_size]\n",
    "\n",
    "val_chord = torch.from_numpy(chord_onehot[:val_size]).float()\n",
    "val_melody = torch.from_numpy(melody[:val_size]).float()\n",
    "val_length = torch.from_numpy(length[:val_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building model...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for CVAE:\n\tsize mismatch for encoder.weight_ih_l0: copying a param with shape torch.Size([2048, 96]) from checkpoint, the shape in current model is torch.Size([4096, 96]).\n\tsize mismatch for encoder.weight_hh_l0: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for encoder.bias_ih_l0: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for encoder.bias_hh_l0: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for encoder.weight_ih_l0_reverse: copying a param with shape torch.Size([2048, 96]) from checkpoint, the shape in current model is torch.Size([4096, 96]).\n\tsize mismatch for encoder.weight_hh_l0_reverse: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for encoder.bias_ih_l0_reverse: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for encoder.bias_hh_l0_reverse: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for encoder.weight_ih_l1: copying a param with shape torch.Size([2048, 1024]) from checkpoint, the shape in current model is torch.Size([4096, 2048]).\n\tsize mismatch for encoder.weight_hh_l1: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for encoder.bias_ih_l1: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for encoder.bias_hh_l1: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for encoder.weight_ih_l1_reverse: copying a param with shape torch.Size([2048, 1024]) from checkpoint, the shape in current model is torch.Size([4096, 2048]).\n\tsize mismatch for encoder.weight_hh_l1_reverse: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for encoder.bias_ih_l1_reverse: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for encoder.bias_hh_l1_reverse: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for encoder_output2mean.weight: copying a param with shape torch.Size([32, 1024]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for encoder_output2mean.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder_output2logv.weight: copying a param with shape torch.Size([32, 1024]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for encoder_output2logv.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for latent2decoder_input.weight: copying a param with shape torch.Size([256, 864]) from checkpoint, the shape in current model is torch.Size([512, 1088]).\n\tsize mismatch for latent2decoder_input.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for decoder.weight_ih_l0: copying a param with shape torch.Size([2048, 256]) from checkpoint, the shape in current model is torch.Size([4096, 512]).\n\tsize mismatch for decoder.weight_hh_l0: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for decoder.bias_ih_l0: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for decoder.bias_hh_l0: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for decoder.weight_ih_l0_reverse: copying a param with shape torch.Size([2048, 256]) from checkpoint, the shape in current model is torch.Size([4096, 512]).\n\tsize mismatch for decoder.weight_hh_l0_reverse: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for decoder.bias_ih_l0_reverse: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for decoder.bias_hh_l0_reverse: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for decoder.weight_ih_l1: copying a param with shape torch.Size([2048, 1024]) from checkpoint, the shape in current model is torch.Size([4096, 2048]).\n\tsize mismatch for decoder.weight_hh_l1: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for decoder.bias_ih_l1: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for decoder.bias_hh_l1: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for decoder.weight_ih_l1_reverse: copying a param with shape torch.Size([2048, 1024]) from checkpoint, the shape in current model is torch.Size([4096, 2048]).\n\tsize mismatch for decoder.weight_hh_l1_reverse: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for decoder.bias_ih_l1_reverse: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for decoder.bias_hh_l1_reverse: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for outputs2chord.weight: copying a param with shape torch.Size([96, 1024]) from checkpoint, the shape in current model is torch.Size([96, 2048]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-7813a656f4f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'building model...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCVAE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'output_models/model_surprisingness_cvae.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m# model.load_state_dict(torch.load('output_models/model_ctdcvae_conv_prenet.pth'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/music/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m    828\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    829\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m--> 830\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m    831\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for CVAE:\n\tsize mismatch for encoder.weight_ih_l0: copying a param with shape torch.Size([2048, 96]) from checkpoint, the shape in current model is torch.Size([4096, 96]).\n\tsize mismatch for encoder.weight_hh_l0: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for encoder.bias_ih_l0: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for encoder.bias_hh_l0: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for encoder.weight_ih_l0_reverse: copying a param with shape torch.Size([2048, 96]) from checkpoint, the shape in current model is torch.Size([4096, 96]).\n\tsize mismatch for encoder.weight_hh_l0_reverse: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for encoder.bias_ih_l0_reverse: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for encoder.bias_hh_l0_reverse: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for encoder.weight_ih_l1: copying a param with shape torch.Size([2048, 1024]) from checkpoint, the shape in current model is torch.Size([4096, 2048]).\n\tsize mismatch for encoder.weight_hh_l1: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for encoder.bias_ih_l1: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for encoder.bias_hh_l1: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for encoder.weight_ih_l1_reverse: copying a param with shape torch.Size([2048, 1024]) from checkpoint, the shape in current model is torch.Size([4096, 2048]).\n\tsize mismatch for encoder.weight_hh_l1_reverse: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for encoder.bias_ih_l1_reverse: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for encoder.bias_hh_l1_reverse: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for encoder_output2mean.weight: copying a param with shape torch.Size([32, 1024]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for encoder_output2mean.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for encoder_output2logv.weight: copying a param with shape torch.Size([32, 1024]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for encoder_output2logv.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for latent2decoder_input.weight: copying a param with shape torch.Size([256, 864]) from checkpoint, the shape in current model is torch.Size([512, 1088]).\n\tsize mismatch for latent2decoder_input.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for decoder.weight_ih_l0: copying a param with shape torch.Size([2048, 256]) from checkpoint, the shape in current model is torch.Size([4096, 512]).\n\tsize mismatch for decoder.weight_hh_l0: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for decoder.bias_ih_l0: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for decoder.bias_hh_l0: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for decoder.weight_ih_l0_reverse: copying a param with shape torch.Size([2048, 256]) from checkpoint, the shape in current model is torch.Size([4096, 512]).\n\tsize mismatch for decoder.weight_hh_l0_reverse: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for decoder.bias_ih_l0_reverse: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for decoder.bias_hh_l0_reverse: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for decoder.weight_ih_l1: copying a param with shape torch.Size([2048, 1024]) from checkpoint, the shape in current model is torch.Size([4096, 2048]).\n\tsize mismatch for decoder.weight_hh_l1: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for decoder.bias_ih_l1: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for decoder.bias_hh_l1: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for decoder.weight_ih_l1_reverse: copying a param with shape torch.Size([2048, 1024]) from checkpoint, the shape in current model is torch.Size([4096, 2048]).\n\tsize mismatch for decoder.weight_hh_l1_reverse: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for decoder.bias_ih_l1_reverse: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for decoder.bias_hh_l1_reverse: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for outputs2chord.weight: copying a param with shape torch.Size([96, 1024]) from checkpoint, the shape in current model is torch.Size([96, 2048])."
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "device = 'cpu'\n",
    "print('building model...')\n",
    "model = CVAE(device = device).to(device)\n",
    "model.load_state_dict(torch.load('output_models/model_surprisingness_cvae.pth'))\n",
    "# model.load_state_dict(torch.load('output_models/model_ctdcvae_conv_prenet.pth'))\n",
    "print(model)\n",
    "model.eval()\n",
    "val_length, val_melody = val_length.to(device), val_melody.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "surp = np.load('./data/surprisingness.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = np.max(surp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Profile function\n",
    "x = np.arange(0,10,1) / norm\n",
    "y = x\n",
    "\n",
    "x = np.arange(10,0,-1) / norm\n",
    "y = x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-32c1c71365a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0msurprisingness\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m34\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0msurprisingness\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msurprisingness_prenet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msurprisingness\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Sampling\n",
    "# torch.manual_seed(0)\n",
    "i = 8\n",
    "sample_melody = val_melody[i].unsqueeze(0)\n",
    "sample_length = val_length[i]\n",
    "sample_melody_framewise = np.expand_dims(val_melody_framewise[i],axis=0)\n",
    "sample_chord_groundtruth_idx = np.expand_dims(val_chord_groundtruth_idx[i],axis=0)\n",
    "sample_tempo = [tempos[i]]\n",
    "sample_downbeat = [downbeats[i]]\n",
    "\n",
    "## Surprising profile\n",
    "s = [3,2,1,0,0,1,2,3]\n",
    "\n",
    "\n",
    "\n",
    "melody_data[i] = np.pad(melody_data[i], ((0, max_melody_len-melody_data[i].shape[0]), (0, 0)), constant_values = (0, 0))\n",
    "\n",
    "surprisingness = torch.Tensor([0] + s * 34).unsqueeze(0).unsqueeze(2)\n",
    "surprisingness = model.surprisingness_prenet(surprisingness)\n",
    "print(sample_length)\n",
    "\n",
    "latent_size = 16\n",
    "\n",
    "# for k in range(10):\n",
    "latent = torch.randn(1,272,latent_size)\n",
    "z = torch.cat([latent,sample_melody,surprisingness], dim=-1)\n",
    "output, chord_pred = model.decode(z)\n",
    "\n",
    "gen_chord_index = torch.max(chord_pred[0][:sample_length],-1).indices\n",
    "print(gen_chord_index)\n",
    "#     print(gen_chord_index.shape)\n",
    "\n",
    "plt.subplot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 272, 96])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chord_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proceed chord decode...\n",
      "accompany_pianoroll shape (1, 272, 128)\n",
      "augment chord into frame base...\n",
      "accompany_pianoroll frame shape: (1, 13056, 128)\n",
      "groundtruth_pianoroll frame shape: (1, 13056, 128)\n",
      "write pianoroll...\n",
      "Finished!\n"
     ]
    }
   ],
   "source": [
    "########## Random sampling ###########\n",
    "# Proceed chord decode\n",
    "print('proceed chord decode...')\n",
    "joint_prob = chord_pred.cpu().detach().numpy()\n",
    "\n",
    "# Append argmax index to get pianoroll array\n",
    "accompany_pianoroll = argmax2pianoroll(joint_prob)\n",
    "\n",
    "# augment chord into frame base\n",
    "BEAT_RESOLUTION = 24\n",
    "BEAT_PER_CHORD = 2\n",
    "\n",
    "accompany_pianoroll_framewise, sample_chord_groundtruth_framewise = sequence2frame(accompany_pianoroll, sample_chord_groundtruth_idx)\n",
    "\n",
    "# length into frame base\n",
    "sample_length_framewise = sample_length * BEAT_RESOLUTION * BEAT_PER_CHORD\n",
    "\n",
    "# write pianoroll\n",
    "result_dir = 'results/surprising_all_cvae'\n",
    "filename = str(i) + '-surprisingness-' + str(s)\n",
    "write_one_pianoroll(result_dir, filename, sample_melody_framewise, accompany_pianoroll_framewise, sample_chord_groundtruth_framewise, sample_length_framewise, sample_tempo, sample_downbeat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 13056, 272, 128)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_chord_groundtruth_framewise.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
